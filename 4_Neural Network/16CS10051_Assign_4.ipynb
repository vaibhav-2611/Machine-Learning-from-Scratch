{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.special import xlogy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules required by Preprocess Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(path):\n",
    "    data = pd.read_csv(path, sep='\\n', header=None)\n",
    "    temp = []\n",
    "    for i in range(len(data)):\n",
    "        x = str(data.iloc[i,0])\n",
    "        temp.append(x.lower())\n",
    "    stop_words = temp\n",
    "    return stop_words\n",
    "\n",
    "def get_train_test_data(data, label, train_percent):\n",
    "    data = pd.DataFrame(data.T)\n",
    "    label = pd.DataFrame(label.T)\n",
    "    m = np.random.rand(len(data)) < train_percent\n",
    "    train = data[m]\n",
    "    train_label = label[m]\n",
    "    test  = data[~m]\n",
    "    test_label = label[~m]\n",
    "    return pd.DataFrame(train.T), pd.DataFrame(test.T), pd.DataFrame(train_label.T), pd.DataFrame(test_label.T)\n",
    "\n",
    "def get_top_tokens(Freq, limit):\n",
    "    res = []\n",
    "    for i in range(min(limit,len(Freq))):\n",
    "        res.append(Freq[i][0])\n",
    "    return res\n",
    "\n",
    "def New_get_All_Tokens(data, stop_words):\n",
    "    L=[]\n",
    "    ps = PorterStemmer()\n",
    "    for row in range(data.shape[0]):\n",
    "        sen = data.iloc[row,1]\n",
    "        sen = sen.lower()\n",
    "        x = New_get_list(sen,stop_words)\n",
    "        L = L+x\n",
    "    X = []\n",
    "    for word in L:\n",
    "        if word in stop_words:\n",
    "            pass\n",
    "        elif ps.stem(word) in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            X.append(ps.stem(word))\n",
    "    L = X\n",
    "    D = dict()\n",
    "    for word in L:\n",
    "        if word in D:\n",
    "            D[word]+=1\n",
    "        else:\n",
    "            D[word]=1\n",
    "    D = sorted(D.items(), key=operator.itemgetter(1))\n",
    "    D.reverse()\n",
    "    return set(L), D\n",
    "\n",
    "def New_get_list(s,stop_words):\n",
    "    ps = PorterStemmer()\n",
    "    start = 0\n",
    "    counter=-1\n",
    "    L=[]\n",
    "    s = s.lower()\n",
    "    for i in range(len(s)):\n",
    "        counter+=1\n",
    "        i = s[i]\n",
    "        if(i==' ' or i=='\\t' or i=='\\n' or i=='.' or i==',' or i==':' or i=='-'):\n",
    "            temp = s[start:counter]\n",
    "            if(len(temp)>0):\n",
    "                L.append(str(temp))\n",
    "            start=counter+1\n",
    "        else:\n",
    "            pass\n",
    "    if(start <len(s) and len(s[start:])>0):\n",
    "        temp = s[start:]\n",
    "        L.append(temp)\n",
    "    return L\n",
    "\n",
    "def New_Get_Input_representation(data, Top_Tokens, stop_words):\n",
    "    DATA  = np.ones((len(Top_Tokens), len(data)))\n",
    "    LABEL = np.ones((1, len(data)))\n",
    "    counter = -1\n",
    "    for row in range(0,data.shape[0]):\n",
    "        counter += 1\n",
    "        X = get_token_of_string(data.iloc[row,1], stop_words)\n",
    "        for i,word in enumerate(Top_Tokens):\n",
    "            if word in X:\n",
    "                DATA[i,counter]=1\n",
    "            else:\n",
    "                DATA[i,counter]=0\n",
    "        if(str(data.iloc[row,0])==\"spam\"):   \n",
    "            LABEL[0,counter] = 1\n",
    "        elif(str(data.iloc[row,0])==\"ham\"):\n",
    "            LABEL[0,counter] = 0\n",
    "        else:\n",
    "            print(\"ERROR in label at row:\",row)\n",
    "    return DATA, LABEL\n",
    "\n",
    "def get_token_of_string(sen, stop_words):\n",
    "    ps = PorterStemmer()\n",
    "    sen = sen.lower()\n",
    "    x = New_get_list(sen,stop_words)\n",
    "    L = x\n",
    "    X = []\n",
    "    for word in L:\n",
    "        if word in stop_words:\n",
    "            pass\n",
    "        elif ps.stem(word) in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            X.append(ps.stem(word))\n",
    "    L = X\n",
    "    return set(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Activation and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Function\n",
    "def F1(x):  \n",
    "    return x\n",
    "\n",
    "# Relu Activation Function\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "# Relu Derivative Function\n",
    "def Relu_der(x):\n",
    "    x = x>0\n",
    "    x = x +0\n",
    "    return x\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "def Sigmoid(x):\n",
    "    s = 1.0/(1.0+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "# Sigmoid Derivative Function\n",
    "def Sigmoid_der(x):\n",
    "    return Sigmoid(x)*(1-Sigmoid(x))\n",
    "\n",
    "# Softmax Activation Function\n",
    "def Softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    s = x_exp/x_sum\n",
    "    return s\n",
    "\n",
    "# Categorical Cross Entropy Loss\n",
    "def Error(X, Y):\n",
    "    E = (xlogy(Y, X) + xlogy(1 - Y, 1 - X))\n",
    "    E = -(np.sum(E))/(X.shape[1])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Module for Preprocess the data\n",
    "def Preprocess(path1, path2, split_ratio, CountTokens):\n",
    "    data = pd.read_csv(path1, sep='\\t', header=None)\n",
    "    stop_words = get_stop_words(path2)\n",
    "    Tokens, Freq = New_get_All_Tokens(data, stop_words)\n",
    "    Top_Tokens = get_top_tokens(Freq, 500)\n",
    "    Dataset, Label = New_Get_Input_representation(data, Top_Tokens, stop_words)\n",
    "    DataSet = pd.DataFrame(Dataset)  # (500, #ex)\n",
    "    Label_  = pd.DataFrame(Label)    # (1, #ex)\n",
    "    Train, Test, Train_Label, Test_Label = get_train_test_data(DataSet, Label_,split_ratio)\n",
    "    Train = pd.DataFrame(Train.values)\n",
    "    Test  = pd.DataFrame(Test.values)\n",
    "    Train_Label = pd.DataFrame(Train_Label.values)\n",
    "    Test_Label = pd.DataFrame(Test_Label.values)    \n",
    "    return Train, Test, Train_Label, Test_Label\n",
    "\n",
    "# Module for Loading the data\n",
    "def DataLoader(Train, Train_Label, batch_size):\n",
    "    df1 = Train\n",
    "    df2 = Train_Label\n",
    "    Result = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    mixed = pd.DataFrame(Result.sample(frac=1, axis=1).values)\n",
    "    size = mixed.shape[1]\n",
    "    batches = math.ceil(size/batch_size)\n",
    "    BATCH=dict()\n",
    "    counter = -1\n",
    "    for i in range(batches):\n",
    "        counter+=1\n",
    "        b = \"batch\"+str(counter)\n",
    "        x = (counter)*batch_size\n",
    "        y = (counter+1)*batch_size\n",
    "        if((counter+1)*batch_size >= size):\n",
    "            y = size\n",
    "        l = [i for i in range(x, y)]\n",
    "        BATCH[b] = mixed[l]\n",
    "    return BATCH\n",
    "\n",
    "# Module for Initialising Weights and Bias\n",
    "def WeightInitialiser(DL1, DL2, DL3):    \n",
    "    W12 = np.random.uniform(-1,1,size=(DL2,DL1)) \n",
    "    W23 = np.random.uniform(-1,1,size=(DL3,DL2))\n",
    "    B12 = np.random.uniform(-1,1, size=(DL2, 1))\n",
    "    B23 = np.random.uniform(-1,1, size=(DL3, 1))\n",
    "    return W12, W23, B12, B23\n",
    "\n",
    "# Module for performing ForwardPropagation\n",
    "def forward(data, data_label, W12, W23, B12, B23):\n",
    "    batch = data.values\n",
    "    Y     = data_label.values\n",
    "    m = batch.shape[1]\n",
    "\n",
    "    assert(batch.shape[1]==Y.shape[1])\n",
    "\n",
    "    S1 = batch\n",
    "    X1 = F1(S1)\n",
    "\n",
    "    S2 = np.dot(W12,X1) + B12\n",
    "    X2 = Relu(S2)\n",
    "\n",
    "    S3 = np.dot(W23,X2) + B23\n",
    "    X3 = Sigmoid(S3)\n",
    "    return S1, X1, S2, X2, S3, X3, m\n",
    "\n",
    "# Module for performing BackwardPropagation\n",
    "def backward(Y, S1, X1, S2, X2, S3, X3, m, W12, W23, B12, B23, alpha):\n",
    "    Y = Y.values\n",
    "    \n",
    "    delta3 = X3-Y\n",
    "    dW23 = np.dot(delta3,X2.T)/m\n",
    "\n",
    "    delta2 = np.dot(W23.T,delta3)*Relu_der(S2)\n",
    "    dW12 = np.dot(delta2, X1.T)/m\n",
    "\n",
    "    db23 = (np.sum(delta3, axis=1, keepdims=True))/m\n",
    "    db12 = (np.sum(delta2, axis=1, keepdims=True))/m\n",
    "\n",
    "    W12 = W12 - alpha*dW12\n",
    "    W23 = W23 - alpha*dW23\n",
    "    B12 = B12 - alpha*db12\n",
    "    B23 = B23 - alpha*db23\n",
    "    return W12, W23, B12, B23\n",
    "\n",
    "# Module to train the model and get train and test accuracies and errors at various epochs\n",
    "def training(Train, Train_Label, DL1, DL2, DL3, alpha, epoch, thresh, Batch_size, Test=None, Test_Label=None):\n",
    "    W12, W23, B12, B23 = WeightInitialiser(DL1, DL2, DL3)\n",
    "    Batch = DataLoader(Train, Train_Label, Batch_size)\n",
    "    report_at = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "    for ITR in range(1,epoch+1):\n",
    "        for k in Batch:\n",
    "            data = Batch[k]\n",
    "            data_train = data[:data.shape[0]-1]\n",
    "            data_label = data[data.shape[0]-1:]\n",
    "            S1, X1, S2, X2, S3, X3, m = forward(data_train, data_label, W12, W23, B12, B23)\n",
    "            W12, W23, B12, B23 = backward(data_label, S1, X1, S2, X2, S3, X3, m, W12, W23, B12, B23, alpha)\n",
    "        if(ITR%100==0 or ITR in report_at):\n",
    "            acc_train = test(Train, Train_Label, W12, W23, B12, B23, thresh)\n",
    "            \n",
    "            s1, x1, s2, x2, s3, x3, m_ = forward(Train, Train_Label, W12, W23, B12, B23)\n",
    "            e  = round(Error(x3, Train_Label.values),5)\n",
    "            \n",
    "            s1, x1, s2, x2, s3, x3, m_ = forward(Test,  Test_Label,  W12, W23, B12, B23)\n",
    "            e2 = round(Error(x3, Test_Label.values),5)\n",
    "            \n",
    "            if(Test is not None and Test_Label is not None):\n",
    "                acc_test = test(Test, Test_Label, W12, W23, B12, B23, thresh)\n",
    "                print(\"Epoch {}=> Train Accu:{}  Test Accu:{}  Train Error:{}  Test Error:{}\".format(ITR,round(acc_train*100,5),round(acc_test*100,5),e,e2))\n",
    "            else:\n",
    "                print(\"Epoch {}=> Train Accu:{}  Train Error:{}\".format(ITR,round(acc_train*100,5),e))\n",
    "    return W12, W23, B12, B23\n",
    "\n",
    "# Module to test the performance\n",
    "def test(Test, Test_Label, W12, W23, B12, B23, thresh):\n",
    "    S1, X1, S2, X2, S3, X3, m = forward(Test, Test_Label, W12, W23, B12, B23)\n",
    "    counter = 0\n",
    "    for i in X3[0]:\n",
    "        if i > thresh:\n",
    "            X3[0][counter]=1\n",
    "        else:\n",
    "            X3[0][counter]=0\n",
    "        counter+=1\n",
    "    correct = np.sum(np.sum(np.logical_not(np.logical_xor(X3,Test_Label))))\n",
    "    accuracy = correct/Test_Label.shape[1]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Finally Training and Testing the Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1=> Train Accu:89.12752  Test Accu:87.56806  Train Error:0.36511  Test Error:0.45006\n",
      "Epoch 2=> Train Accu:92.70694  Test Accu:91.65154  Train Error:0.23144  Test Error:0.28578\n",
      "Epoch 3=> Train Accu:94.22819  Test Accu:92.92196  Train Error:0.18101  Test Error:0.22425\n",
      "Epoch 4=> Train Accu:94.98881  Test Accu:94.19238  Train Error:0.1536  Test Error:0.19189\n",
      "Epoch 5=> Train Accu:95.68233  Test Accu:94.73684  Train Error:0.13603  Test Error:0.17203\n",
      "Epoch 6=> Train Accu:96.08501  Test Accu:95.19056  Train Error:0.12362  Test Error:0.15905\n",
      "Epoch 7=> Train Accu:96.37584  Test Accu:95.46279  Train Error:0.11446  Test Error:0.15035\n",
      "Epoch 8=> Train Accu:96.73378  Test Accu:95.82577  Train Error:0.10738  Test Error:0.14424\n",
      "Epoch 9=> Train Accu:96.95749  Test Accu:96.18875  Train Error:0.1016  Test Error:0.13962\n",
      "Epoch 10=> Train Accu:97.11409  Test Accu:96.37024  Train Error:0.09665  Test Error:0.13592\n",
      "Epoch 20=> Train Accu:98.12081  Test Accu:97.00544  Train Error:0.06715  Test Error:0.11812\n",
      "Epoch 30=> Train Accu:98.81432  Test Accu:97.54991  Train Error:0.05089  Test Error:0.11257\n",
      "Epoch 40=> Train Accu:99.23937  Test Accu:97.64065  Train Error:0.04024  Test Error:0.11169\n",
      "Epoch 50=> Train Accu:99.3736  Test Accu:97.91289  Train Error:0.03287  Test Error:0.1133\n",
      "Epoch 60=> Train Accu:99.46309  Test Accu:98.00363  Train Error:0.02742  Test Error:0.11594\n",
      "Epoch 70=> Train Accu:99.57494  Test Accu:98.00363  Train Error:0.02312  Test Error:0.11938\n",
      "Epoch 80=> Train Accu:99.6868  Test Accu:98.00363  Train Error:0.01962  Test Error:0.12342\n",
      "Epoch 90=> Train Accu:99.70917  Test Accu:98.00363  Train Error:0.01666  Test Error:0.12798\n",
      "Epoch 100=> Train Accu:99.70917  Test Accu:98.00363  Train Error:0.01415  Test Error:0.13303\n",
      "Epoch 200=> Train Accu:99.91051  Test Accu:97.82214  Train Error:0.00539  Test Error:0.17216\n",
      "Epoch 300=> Train Accu:99.91051  Test Accu:97.64065  Train Error:0.00398  Test Error:0.18901\n",
      "Epoch 400=> Train Accu:99.91051  Test Accu:97.64065  Train Error:0.00345  Test Error:0.19866\n",
      "Epoch 500=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00317  Test Error:0.2057\n",
      "Epoch 600=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.003  Test Error:0.21136\n",
      "Epoch 700=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00289  Test Error:0.21616\n",
      "Epoch 800=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00282  Test Error:0.22037\n",
      "Epoch 900=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00276  Test Error:0.22409\n",
      "Epoch 1000=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00272  Test Error:0.22738\n",
      "Epoch 1100=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00269  Test Error:0.23029\n",
      "Epoch 1200=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00266  Test Error:0.23301\n",
      "Epoch 1300=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00264  Test Error:0.23544\n",
      "Epoch 1400=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00262  Test Error:0.23763\n",
      "Epoch 1500=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.0026  Test Error:0.23971\n",
      "Epoch 1600=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00259  Test Error:0.24154\n",
      "Epoch 1700=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00258  Test Error:0.24331\n",
      "Epoch 1800=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00257  Test Error:0.24498\n",
      "Epoch 1900=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00256  Test Error:0.24653\n",
      "Epoch 2000=> Train Accu:99.91051  Test Accu:97.54991  Train Error:0.00255  Test Error:0.24799\n",
      "Test Accuracy: 97.54990925589837\n"
     ]
    }
   ],
   "source": [
    "# PreProcessing\n",
    "split_ratio = 0.8 # 80% training 20% test\n",
    "Freq_limit  = 500 # top 500 in frequencies\n",
    "Train, Test, Train_Label, Test_Label = Preprocess('Assignment_4_data.txt', 'stop_words.txt', split_ratio , Freq_limit)\n",
    "\n",
    "# Training\n",
    "d1 = 500             # Number of neurons in input layer\n",
    "d2 = 100             # Number of neurons in hidden layer\n",
    "d3 = 1               # Number of neurons in output layer\n",
    "alpha = 0.1          # Learning Rate\n",
    "Num_Epochs = 2000    # Number of Epochs\n",
    "Threshold = 0.5      # Threshold for classifying HAM or SPAM\n",
    "Batch_size  = 50     # Batch Size for creating batches in DataLoader Module\n",
    "\n",
    "W12, W23, B12, B23 = training(Train, Train_Label, d1, d2, d3, alpha, Num_Epochs, Threshold, Batch_size, Test, Test_Label)\n",
    "\n",
    "# Testing\n",
    "Test_Acc = test(Test, Test_Label, W12, W23, B12, B23, Threshold)\n",
    "print(\"Test Accuracy: {}\".format(Test_Acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for Preprocess the data\n",
    "def Preprocess2(path1, path2, split_ratio, CountTokens):\n",
    "    data = pd.read_csv(path1, sep='\\t', header=None)\n",
    "    stop_words = get_stop_words(path2)\n",
    "    Tokens, Freq = New_get_All_Tokens(data, stop_words)\n",
    "    Top_Tokens = get_top_tokens(Freq, 500)\n",
    "    Dataset, Label = New_Get_Input_representation(data, Top_Tokens, stop_words)\n",
    "    \n",
    "    DataSet = pd.DataFrame(Dataset)  # (500, #ex)\n",
    "    Label_  = pd.DataFrame(Label)    # (1, #ex)\n",
    "    Train, Test, Train_Label, Test_Label = get_train_test_data(DataSet, Label_,split_ratio)\n",
    "    Train = pd.DataFrame(Train.values)\n",
    "    Test  = pd.DataFrame(Test.values)\n",
    "    Train_Label = pd.DataFrame(Train_Label.values)\n",
    "    Test_Label = pd.DataFrame(Test_Label.values)    \n",
    "    return Train, Test, Train_Label, Test_Label\n",
    "\n",
    "# Module for Loading the data\n",
    "def DataLoader2(Train, Train_Label, batch_size):\n",
    "    df1 = Train\n",
    "    df2 = Train_Label\n",
    "    Result = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    mixed = pd.DataFrame(Result.sample(frac=1, axis=1).values)\n",
    "    size = mixed.shape[1]\n",
    "    batches = math.ceil(size/batch_size)\n",
    "    BATCH=dict()\n",
    "    counter = -1\n",
    "    for i in range(batches):\n",
    "        counter+=1\n",
    "        b = \"batch\"+str(counter)\n",
    "        x = (counter)*batch_size\n",
    "        y = (counter+1)*batch_size\n",
    "        if((counter+1)*batch_size >= size):\n",
    "            y = size\n",
    "        l = [i for i in range(x, y)]\n",
    "        BATCH[b] = mixed[l]\n",
    "    return BATCH\n",
    "\n",
    "# Module for Initialising Weights and Bias\n",
    "def WeightInitialiser2(DL1, DL2, DL3, DL4):    \n",
    "    W12 = np.random.uniform(-1,1,size=(DL2,DL1))\n",
    "    W23 = np.random.uniform(-1,1,size=(DL3,DL2))\n",
    "    W34 = np.random.uniform(-1,1,size=(DL4,DL3))\n",
    "    B12 = np.random.uniform(-1,1, size=(DL2, 1))\n",
    "    B23 = np.random.uniform(-1,1, size=(DL3, 1))\n",
    "    B34 = np.random.uniform(-1,1, size=(DL4, 1))\n",
    "    return W12, W23, W34, B12, B23, B34\n",
    "\n",
    "# Module for performing ForwardPropagation\n",
    "def forward2(data, data_label, W12, W23, W34, B12, B23, B34):\n",
    "    batch = data.values\n",
    "    Y     = data_label.values\n",
    "    m     = batch.shape[1]\n",
    "\n",
    "    assert(batch.shape[1]==Y.shape[1])\n",
    "\n",
    "    S1 = batch\n",
    "    X1 = S1\n",
    "\n",
    "    S2 = np.dot(W12,X1) + B12\n",
    "    X2 = Sigmoid(S2)\n",
    "\n",
    "    S3 = np.dot(W23,X2) + B23\n",
    "    X3 = Sigmoid(S3)\n",
    "\n",
    "    S4 = np.dot(W34,X3) + B34\n",
    "    X4 = Softmax(S4)\n",
    "\n",
    "    return S1, X1, S2, X2, S3, X3, S4, X4, m\n",
    "\n",
    "# Module for performing BackwardPropagation\n",
    "def backward2(Y, S1, X1, S2, X2, S3, X3, S4, X4, m, W12, W23, W34, B12, B23, B34, alpha):\n",
    "    Y = Y.values\n",
    "    Y = np.array([Y[0],1-Y[0]])\n",
    "     \n",
    "    delta4 = X4-Y\n",
    "    dW34 = np.dot(delta4,X3.T)/m\n",
    "\n",
    "    delta3 = np.dot(W34.T,delta4)*Sigmoid_der(S3)\n",
    "    dW23 = np.dot(delta3, X2.T)/m\n",
    "\n",
    "    delta2 = np.dot(W23.T,delta3)*Sigmoid_der(S2)\n",
    "    dW12 = np.dot(delta2, X1.T)/m\n",
    "\n",
    "    db34 = (np.sum(delta4, axis=1, keepdims=True))/m\n",
    "    db23 = (np.sum(delta3, axis=1, keepdims=True))/m\n",
    "    db12 = (np.sum(delta2, axis=1, keepdims=True))/m\n",
    "\n",
    "    W12 = W12 - alpha*dW12\n",
    "    W23 = W23 - alpha*dW23\n",
    "    W34 = W34 - alpha*dW34\n",
    "    B12 = B12 - alpha*db12\n",
    "    B23 = B23 - alpha*db23\n",
    "    B34 = B34 - alpha*db34\n",
    "    return W12, W23, W34, B12, B23, B34\n",
    "\n",
    "# Module to train the model and get train and test accuracies and errors at various epochs\n",
    "def training2(Train, Train_Label, DL1, DL2, DL3, DL4, alpha, epoch, thresh, Batch_size, Test=None, Test_Label=None):\n",
    "    W12, W23, W34, B12, B23, B34 = WeightInitialiser2(DL1, DL2, DL3, DL4)\n",
    "    Batch = DataLoader2(Train, Train_Label, Batch_size)\n",
    "    report_at = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "    for ITR in range(1,epoch+1):\n",
    "        for k in Batch:\n",
    "            data = Batch[k]\n",
    "            data_train = data[:data.shape[0]-1]\n",
    "            data_label = data[data.shape[0]-1:]\n",
    "            S1, X1, S2, X2, S3, X3, S4, X4, m = forward2(data_train, data_label, W12, W23, W34, B12, B23, B34)\n",
    "            W12, W23, W34, B12, B23, B34 = backward2(data_label, S1, X1, S2, X2, S3, X3, S4, X4, m, W12, W23, W34, B12, B23, B34, alpha)\n",
    "        if(ITR%100==0 or ITR in report_at):\n",
    "            acc_train = test2(Train, Train_Label, W12, W23, W34, B12, B23, B34, thresh)\n",
    "\n",
    "            s1, x1, s2, x2, s3, x3, s4, x4, m = forward2(Train, Train_Label, W12, W23, W34, B12, B23, B34) \n",
    "            Y = np.array([Train_Label.values[0],1-Train_Label.values[0]])\n",
    "            e = -np.sum(xlogy(Y,x4))/x4.shape[1]\n",
    "            \n",
    "            s1, x1, s2, x2, s3, x3, s4, x4, m = forward2(Test, Test_Label, W12, W23, W34, B12, B23, B34)\n",
    "            Y  = np.array([Test_Label.values[0],1-Test_Label.values[0]])\n",
    "            e2 = -np.sum(xlogy(Y,x4))/x4.shape[1]\n",
    "            \n",
    "            if(Test is not None and Test_Label is not None):\n",
    "                acc_test = test2(Test, Test_Label, W12, W23, W34, B12, B23, B34, thresh)\n",
    "                print(\"Epoch {}=> Train Accu:{}  Test Accu:{}  Train Error:{}  Test Error:{}\".format(ITR,round(acc_train*100,5),round(acc_test*100,5),round(e,5),round(e2,5)))\n",
    "            else:\n",
    "                print(\"Epoch {}=> Train Accu:{}  Train Error:{}\".format(ITR,round(acc_train*100,5),round(e,5)))\n",
    "    return W12, W23, W34, B12, B23, B34\n",
    "\n",
    "# Module to test the performance\n",
    "def test2(Test, Test_Label, W12, W23, W34, B12, B23, B34, thresh):\n",
    "    S1, X1, S2, X2, S3, X3, S4, X4, m = forward2(Test, Test_Label, W12, W23, W34, B12, B23, B34)\n",
    "    Y = np.array([Test_Label.values[0],1-Test_Label.values[0]])\n",
    "    size = X4.shape[1]\n",
    "    assert(size == Test_Label.shape[1])\n",
    "    correct = 0\n",
    "    for i in range(size):\n",
    "        if X4[0][i] > X4[1][i] and Y[0][i]==1:\n",
    "            correct+=1\n",
    "        elif X4[0][i] <= X4[1][i] and Y[1][i]==1:\n",
    "            correct+=1\n",
    "        else:\n",
    "            pass\n",
    "    accuracy = correct/size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Finally Training and Testing the Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1=> Train Accu:86.73884  Test Accu:85.67663  Train Error:0.36079  Test Error:0.37925\n",
      "Epoch 2=> Train Accu:86.8516  Test Accu:85.7645  Train Error:0.34298  Test Error:0.36031\n",
      "Epoch 3=> Train Accu:87.07713  Test Accu:86.02812  Train Error:0.32671  Test Error:0.34284\n",
      "Epoch 4=> Train Accu:87.59585  Test Accu:86.64323  Train Error:0.31152  Test Error:0.32642\n",
      "Epoch 5=> Train Accu:88.20478  Test Accu:87.52197  Train Error:0.29725  Test Error:0.3109\n",
      "Epoch 6=> Train Accu:88.81371  Test Accu:87.87346  Train Error:0.28386  Test Error:0.29626\n",
      "Epoch 7=> Train Accu:89.53541  Test Accu:88.4007  Train Error:0.27137  Test Error:0.28253\n",
      "Epoch 8=> Train Accu:90.18945  Test Accu:89.27944  Train Error:0.2598  Test Error:0.26977\n",
      "Epoch 9=> Train Accu:90.55029  Test Accu:89.63093  Train Error:0.24911  Test Error:0.25796\n",
      "Epoch 10=> Train Accu:91.15922  Test Accu:90.94903  Train Error:0.23926  Test Error:0.24708\n",
      "Epoch 20=> Train Accu:93.91069  Test Accu:93.93673  Train Error:0.17277  Test Error:0.17488\n",
      "Epoch 30=> Train Accu:95.62472  Test Accu:95.51845  Train Error:0.13841  Test Error:0.13988\n",
      "Epoch 40=> Train Accu:96.32386  Test Accu:96.22144  Train Error:0.11773  Test Error:0.12066\n",
      "Epoch 50=> Train Accu:96.77492  Test Accu:96.66081  Train Error:0.10336  Test Error:0.10846\n",
      "Epoch 60=> Train Accu:97.18088  Test Accu:96.83656  Train Error:0.09235  Test Error:0.09977\n",
      "Epoch 70=> Train Accu:97.54172  Test Accu:97.0123  Train Error:0.08345  Test Error:0.09311\n",
      "Epoch 80=> Train Accu:97.63194  Test Accu:97.0123  Train Error:0.07605  Test Error:0.08777\n",
      "Epoch 90=> Train Accu:97.99278  Test Accu:97.45167  Train Error:0.06981  Test Error:0.08338\n",
      "Epoch 100=> Train Accu:98.19576  Test Accu:97.71529  Train Error:0.06449  Test Error:0.07971\n",
      "Epoch 200=> Train Accu:99.09788  Test Accu:98.41828  Train Error:0.03552  Test Error:0.06283\n",
      "Epoch 300=> Train Accu:99.41362  Test Accu:98.24253  Train Error:0.02217  Test Error:0.06145\n",
      "Epoch 400=> Train Accu:99.68426  Test Accu:98.24253  Train Error:0.01527  Test Error:0.06617\n",
      "Epoch 500=> Train Accu:99.84213  Test Accu:98.41828  Train Error:0.01174  Test Error:0.07191\n",
      "Epoch 600=> Train Accu:99.88724  Test Accu:98.24253  Train Error:0.00974  Test Error:0.07707\n",
      "Epoch 700=> Train Accu:99.88724  Test Accu:98.24253  Train Error:0.00847  Test Error:0.0817\n",
      "Epoch 800=> Train Accu:99.88724  Test Accu:98.3304  Train Error:0.00762  Test Error:0.08573\n",
      "Epoch 900=> Train Accu:99.88724  Test Accu:98.15466  Train Error:0.00703  Test Error:0.08908\n",
      "Epoch 1000=> Train Accu:99.90979  Test Accu:98.15466  Train Error:0.00659  Test Error:0.09186\n",
      "Epoch 1100=> Train Accu:99.90979  Test Accu:98.15466  Train Error:0.00625  Test Error:0.09421\n",
      "Epoch 1200=> Train Accu:99.90979  Test Accu:98.15466  Train Error:0.00597  Test Error:0.09624\n",
      "Epoch 1300=> Train Accu:99.90979  Test Accu:98.15466  Train Error:0.00574  Test Error:0.09805\n",
      "Epoch 1400=> Train Accu:99.90979  Test Accu:98.06678  Train Error:0.00554  Test Error:0.09969\n",
      "Epoch 1500=> Train Accu:99.90979  Test Accu:98.06678  Train Error:0.00537  Test Error:0.10121\n",
      "Epoch 1600=> Train Accu:99.90979  Test Accu:97.97891  Train Error:0.00521  Test Error:0.10264\n",
      "Epoch 1700=> Train Accu:99.90979  Test Accu:97.97891  Train Error:0.00507  Test Error:0.10402\n",
      "Epoch 1800=> Train Accu:99.90979  Test Accu:97.97891  Train Error:0.00495  Test Error:0.10535\n",
      "Epoch 1900=> Train Accu:99.90979  Test Accu:97.97891  Train Error:0.00483  Test Error:0.10665\n",
      "Epoch 2000=> Train Accu:99.90979  Test Accu:97.97891  Train Error:0.00473  Test Error:0.10792\n",
      "Test Accuracy: 97.97891036906854\n"
     ]
    }
   ],
   "source": [
    "# PreProcessing\n",
    "split_ratio = 0.8 # 80% training 20% test\n",
    "Freq_limit  = 500 # top 500 in frequencies\n",
    "Train, Test, Train_Label, Test_Label = Preprocess2('Assignment_4_data.txt', 'stop_words.txt', split_ratio , Freq_limit)\n",
    "\n",
    "# Training\n",
    "d1 = 500             # Number of neurons in input layer\n",
    "d2 = 100             # Number of neurons in hidden layer 1\n",
    "d3 = 20              # Number of neurons in hidden layer 2\n",
    "d4 = 2               # Number of neurons in hidden layer 2\n",
    "alpha = 0.1          # Learning Rate\n",
    "Num_Epochs = 2000    # Number of Epochs\n",
    "Threshold = 0.5      # Threshold for classifying HAM or SPAM\n",
    "Batch_size  = 100    # Batch Size for creating batches in DataLoader Module\n",
    "W12, W23, W34, B12, B23, B34 = training2(Train, Train_Label, d1, d2, d3, d4, alpha, Num_Epochs, Threshold, Batch_size, Test, Test_Label)\n",
    "\n",
    "# Testing\n",
    "Test_Acc = test2(Test, Test_Label, W12, W23, W34, B12, B23, B34, Threshold)\n",
    "print(\"Test Accuracy: {}\".format(Test_Acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
